# Use Flax linen to define a ConvNet architecture (Conv -> BatchNorm -> CustomLayer -> Dense)
# Add optional dropout, softmax at end
# Train using Adam + cross-entropy loss
# Track metrics (loss, accuracy) per epoch